{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "import torch                    \n",
    "import os\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import *\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import pretrainedmodels\n",
    "import glob\n",
    "import faiss\n",
    "\n",
    "import train_get_acc as tacc\n",
    "import dill as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path= glob.glob('../train/*/*')\n",
    "num=glob.glob('../train/*')\n",
    "\n",
    "img_label=[float(img_path.split('/')[-2]) for img_path in path]\n",
    "\n",
    "label_len=len(num)\n",
    "\n",
    "query_imgs_path =glob.glob('../query_frame/*/*.jpg')\n",
    "query_imgs_path.sort(key=lambda x: x.lower())\n",
    "\n",
    "refer_imgs_path = glob.glob('../refer_frame/*/*.jpg')\n",
    "refer_imgs_path.sort(key=lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../refer_frame_train_20/1244627000/1244627000_00001.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(refer_imgs_path[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../train/1/2a2645c4-b942-11e9-8b81-fa163ee49799_00016.jpg', '../train/1/2a2645c4-b942-11e9-8b81-fa163ee49799_00017.jpg', '../train/1/2a2645c4-b942-11e9-8b81-fa163ee49799_00018.jpg', '../train/1/2a2645c4-b942-11e9-8b81-fa163ee49799_00019.jpg', '../train/3/2a2645c4-b942-11e9-8b81-fa163ee49799_00055.jpg', '../train/3/2a2645c4-b942-11e9-8b81-fa163ee49799_00054.jpg', '../train/3/2a2645c4-b942-11e9-8b81-fa163ee49799_00056.jpg', '../train/2/2a2645c4-b942-11e9-8b81-fa163ee49799_00033.jpg', '../train/2/2a2645c4-b942-11e9-8b81-fa163ee49799_00032.jpg', '../train/2/2a2645c4-b942-11e9-8b81-fa163ee49799_00031.jpg', '../train/0/2a2645c4-b942-11e9-8b81-fa163ee49799_00004.jpg', '../train/0/2a2645c4-b942-11e9-8b81-fa163ee49799_00002.jpg', '../train/0/2a2645c4-b942-11e9-8b81-fa163ee49799_00003.jpg', '../train/0/2a2645c4-b942-11e9-8b81-fa163ee49799_00001.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_dataset(Dataset):\n",
    "    def __init__(self, path,label,transform=None):\n",
    "        \n",
    "        self.img_paths=path\n",
    "        \n",
    "        self.img_label=label\n",
    "        if transform is not None:\n",
    "            self.transform=transform\n",
    "        else:\n",
    "            self.transform=None\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.img_paths[index])\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img,self.img_label[index]\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "class val_dataset(Dataset):\n",
    "    def __init__(self, img_path, transform=None):\n",
    "        self.img_path = img_path\n",
    "#         print(self.img_path)\n",
    "        self.img_label = np.zeros(len(img_path))\n",
    "    \n",
    "        if transform is not None:\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = None\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.img_path[index])\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, self.img_path[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predit_file(path):\n",
    "    val_model=model\n",
    "#     print(val_model)\n",
    "    del val_model.Linear_layer\n",
    "    val_model.Linear_layer=lambda x:x\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    val_model=val_model.to(device)\n",
    "    if not isinstance(path, list):\n",
    "        path = [path]\n",
    "    \n",
    "    # print(path)\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset(path, \n",
    "                transforms.Compose([\n",
    "#                             transforms.Resize((256, 256)),  # 先调整图片大小至256x256\n",
    "#                             transforms.RandomCrop((224, 224)),\n",
    "                            transforms.Resize((224, 224)),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        ), batch_size=40, shuffle=False, num_workers=0,\n",
    "    )\n",
    "    \n",
    "    img_feat = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data in data_loader:\n",
    "            batch_x, batch_y = batch_data\n",
    "#             print(batch_x)\n",
    "#             print(batch_y)\n",
    "            \n",
    "            # print(batch_y[:10])\n",
    "            batch_x = Variable(batch_x).cuda()\n",
    "            feat_pred = val_model(batch_x)\n",
    "#             print(feat_pred)\n",
    "\n",
    "            # max-pooling\n",
    "            # feat_pred = F.max_pool2d(feat_pred, kernel_size=(24, 32))\n",
    "            \n",
    "            # ave-pooling\n",
    "            # feat_pred = F.avg_pool2d(feat_pred, kernel_size=(24, 32))[:, :, 0, 0]\n",
    "            \n",
    "            #print(feat_pred.shape, batch_x.shape)\n",
    "            feat_pred = feat_pred.data.cpu().numpy()\n",
    "            # feat_pred = feat_pred.max(-1).max(-1)\n",
    "            \n",
    "            # feat_pred = feat_pred.reshape((-1, 512))\n",
    "            img_feat.append(feat_pred)\n",
    "            \n",
    "            del feat_pred\n",
    "            # img_feat.append(feat_pred)\n",
    "            \n",
    "    img_feat = np.vstack(img_feat)\n",
    "    return img_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改之后的网络\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(Net, self).__init__()\n",
    "        # 取掉model的后1层\n",
    "        self.resnet_layer = nn.Sequential(*list(model.children())[:-1])\n",
    "        self.Linear_layer = nn.Linear(2048, int(label_len)) #加上一层参数修改好的全连接层\n",
    "    def forward(self, x):\n",
    "        x = self.resnet_layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.Linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 先调整图片大小至256x256\n",
    "#     transforms.RandomCrop((224, 224)),  # 再随机裁剪到224x224\n",
    "#     transforms.RandomHorizontalFlip(),  # 随机的图像水平翻转，通俗讲就是图像的左右对调\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.2225))  # 归一化，数值是用ImageNet给出的数值\n",
    "])\n",
    "trainset = train_dataset(path, img_label,transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=False, num_workers=0)\n",
    "\n",
    "def train(epoch):\n",
    "    print('第'+str(epoch)+'轮训练')\n",
    "    scheduler.step()\n",
    "    model.train()\n",
    "    for id ,(img,label) in enumerate(trainloader):\n",
    "#         img,label=data\n",
    "#         img.type(torch.FloatTensor)\n",
    "        torch.as_tensor(img, dtype=float)\n",
    "#         print(len(img))\n",
    "        label=np.array(label,dtype=float)\n",
    "        label=torch.from_numpy(label)\n",
    "        image = Variable(img).cuda()\n",
    "        label = Variable(label).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(image)\n",
    "#         print('out:{}'.format(out))\n",
    "#         print(out.shape)\n",
    "#         print('label:{}'.format(label))\n",
    "        loss = criterion(out, label.long())\n",
    "        print('loss:{0}'.format(loss))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, codecs\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "# import cv2\n",
    "\n",
    "from sklearn.preprocessing import normalize as sknormalize\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def normalize(x, copy=False):\n",
    "    \"\"\"\n",
    "    A helper function that wraps the function of the same name in sklearn.\n",
    "    This helper handles the case of a single column vector.\n",
    "    \"\"\"\n",
    "    if type(x) == np.ndarray and len(x.shape) == 1:\n",
    "        return np.squeeze(sknormalize(x.reshape(1,-1), copy=copy))\n",
    "        #return np.squeeze(x / np.sqrt((0.28864568x ** 2).sum(-1))[..., np.newaxis])\n",
    "    else:\n",
    "        return sknormalize(x, copy=copy)\n",
    "        #return x / np.sqrt((x ** 2).sum(-1))[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_predict():\n",
    "    result=[]\n",
    "    query_cnn = predit_file(list(query_imgs_path[0:2]))\n",
    "    refer_cnn = predit_file(list(refer_imgs_path[0:2]))\n",
    "    query_cnn = normalize(query_cnn)\n",
    "    refer_cnn = normalize(refer_cnn)\n",
    "    \n",
    "    d=int(query_cnn.shape[1])\n",
    "    nlist = 1                      #聚类中心的个数\n",
    "    k = 5\n",
    "    quantizer = faiss.IndexFlatL2(d)  # the other index\n",
    "    index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "    # assert not index.is_trained\n",
    "    index.train(refer_cnn)\n",
    "    # assert index.is_trained\n",
    "\n",
    "    index.add(refer_cnn)                  # add may be a bit slower as well\n",
    "    D1, I1 = index.search(query_cnn, k)     # actual search\n",
    "\n",
    "    index.nprobe = 10              # default nprobe is 1, try a few more\n",
    "    D2, I2 = index.search(query_cnn, k)\n",
    "    \n",
    "    num=query_cnn.shape[0]\n",
    "    for i in range(num):\n",
    "        for id,j in enumerate(I1[i]):\n",
    "            if(D1[i][id]>0.8):\n",
    "                data=str(query_imgs_path[i].split('/')[-1])+' '+str(refer_imgs_path[j].split('/')[-1])+' '+str(D1[i][id])\n",
    "                result.append(data)\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0轮训练\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cyf/anaconda3/envs/py3.5/lib/python3.5/site-packages/torch/optim/lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:1.5741569995880127\n",
      "loss:1.8909828662872314\n",
      "['21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00001.jpg 0.82160395', '21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00002.jpg 0.8103409', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00002.jpg 0.84265405', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00001.jpg 0.8367941']\n",
      "500.0\n",
      "第1轮训练\n",
      "loss:7.7504401206970215\n",
      "loss:7.6771650314331055\n",
      "['21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00001.jpg 0.8669853', '21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00002.jpg 0.8514081', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00001.jpg 0.8740861', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00002.jpg 0.8542563']\n",
      "500.0\n",
      "第2轮训练\n",
      "loss:5.9022979736328125\n",
      "loss:7.219998836517334\n",
      "['21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00001.jpg 0.8664949', '21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00002.jpg 0.8536371', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00001.jpg 0.8746674', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00002.jpg 0.85636926']\n",
      "500.0\n",
      "第3轮训练\n",
      "loss:5.4235734939575195\n",
      "loss:7.148805141448975\n",
      "['21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00001.jpg 0.866933', '21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00002.jpg 0.85363436', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00001.jpg 0.8749142', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00002.jpg 0.8569052']\n",
      "500.0\n",
      "第4轮训练\n",
      "loss:4.921784400939941\n",
      "loss:7.053396701812744\n",
      "['21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00001.jpg 0.8676022', '21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00002.jpg 0.8516386', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00001.jpg 0.8731743', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00002.jpg 0.856253']\n",
      "500.0\n",
      "第5轮训练\n",
      "loss:4.478146076202393\n",
      "loss:6.99544620513916\n",
      "['21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00001.jpg 0.8676342', '21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00002.jpg 0.85144526', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00001.jpg 0.87288266', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00002.jpg 0.85610056']\n",
      "500.0\n",
      "第6轮训练\n",
      "loss:4.430464744567871\n",
      "loss:6.984557628631592\n",
      "['21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00001.jpg 0.8676375', '21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00002.jpg 0.85125697', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00001.jpg 0.87251437', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00002.jpg 0.8559673']\n",
      "500.0\n",
      "第7轮训练\n",
      "loss:4.374612331390381\n",
      "loss:6.972409725189209\n",
      "['21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00001.jpg 0.8675699', '21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00002.jpg 0.85104793', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00001.jpg 0.87209177', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00002.jpg 0.85589087']\n",
      "500.0\n",
      "第8轮训练\n",
      "loss:4.314156532287598\n",
      "loss:6.96487283706665\n",
      "['21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00001.jpg 0.8675569', '21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00002.jpg 0.8510262', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00001.jpg 0.87204695', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00002.jpg 0.8558824']\n",
      "500.0\n",
      "第9轮训练\n",
      "loss:4.307840824127197\n",
      "loss:6.9634552001953125\n",
      "['21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00001.jpg 0.86754096', '21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00002.jpg 0.8510053', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00001.jpg 0.87200093', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00002.jpg 0.85587287']\n",
      "500.0\n",
      "第10轮训练\n",
      "loss:4.301158428192139\n",
      "loss:6.961952209472656\n",
      "['21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00001.jpg 0.8675227', '21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00002.jpg 0.8509854', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00001.jpg 0.8719537', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00002.jpg 0.8558628']\n",
      "500.0\n",
      "第11轮训练\n",
      "loss:4.294204235076904\n",
      "loss:6.961050033569336\n",
      "['21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00001.jpg 0.86752105', '21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00002.jpg 0.8509833', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00001.jpg 0.87194866', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00002.jpg 0.85586166']\n",
      "500.0\n",
      "第12轮训练\n",
      "loss:4.293484687805176\n",
      "loss:6.960885524749756\n",
      "['21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00001.jpg 0.86751866', '21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00002.jpg 0.85098124', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00001.jpg 0.8719436', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00002.jpg 0.85586035']\n",
      "500.0\n",
      "第13轮训练\n",
      "loss:4.292742729187012\n",
      "loss:6.960715293884277\n",
      "['21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00001.jpg 0.86751664', '21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00002.jpg 0.8509793', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00001.jpg 0.8719388', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00002.jpg 0.85585886']\n",
      "500.0\n",
      "第14轮训练\n",
      "loss:4.2919816970825195\n",
      "loss:6.960614204406738\n",
      "['21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00001.jpg 0.8675162', '21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00002.jpg 0.8509793', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00001.jpg 0.8719382', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00002.jpg 0.8558587']\n",
      "500.0\n",
      "第15轮训练\n",
      "loss:4.291903495788574\n",
      "loss:6.960596561431885\n",
      "['21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00001.jpg 0.86751604', '21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00002.jpg 0.8509787', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00001.jpg 0.87193763', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00002.jpg 0.85585856']\n",
      "500.0\n",
      "第16轮训练\n",
      "loss:4.291825294494629\n",
      "loss:6.960578441619873\n",
      "['21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00001.jpg 0.8675157', '21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00002.jpg 0.85097855', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00001.jpg 0.8719373', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00002.jpg 0.8558584']\n",
      "500.0\n",
      "第17轮训练\n",
      "loss:4.291744709014893\n",
      "loss:6.960567951202393\n",
      "['21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00001.jpg 0.8675157', '21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00002.jpg 0.8509785', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00001.jpg 0.87193704', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00002.jpg 0.8558583']\n",
      "500.0\n",
      "第18轮训练\n",
      "loss:4.291738033294678\n",
      "loss:6.96056604385376\n",
      "['21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00001.jpg 0.86751574', '21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00002.jpg 0.85097855', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00001.jpg 0.87193716', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00002.jpg 0.8558585']\n",
      "500.0\n",
      "第19轮训练\n",
      "loss:4.291731834411621\n",
      "loss:6.960564613342285\n",
      "['21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00001.jpg 0.8675157', '21443adc-b911-11e9-ad99-fa163ee49799_00001.jpg 1244627000_00002.jpg 0.8509785', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00001.jpg 0.8719371', '21443adc-b911-11e9-ad99-fa163ee49799_00002.jpg 1244627000_00002.jpg 0.85585845']\n",
      "500.0\n"
     ]
    }
   ],
   "source": [
    "model_name = 'se_resnet50' # could be fbresnet152 or inceptionresnetv2\n",
    "model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')\n",
    "model.eval()\n",
    "# del model.last_linear\n",
    "# model.last_linear=lambda x:x\n",
    "model=Net(model)\n",
    "device = torch.device(\"cuda\")\n",
    "model=model.to(device)\n",
    "\n",
    "# for param in transfer_model.parameters():\n",
    "#     param.require_grad = true\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)  # 设置训练细节\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.99))  # 设置训练细节\n",
    "scheduler = StepLR(optimizer, step_size=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# for epoch in range(int(label_len)):\n",
    "\n",
    "acc_log=[]\n",
    "for epoch in range(20):\n",
    "    train(epoch)\n",
    "#     torch.save(model, './'+str(epoch)+'_.pth')  # 保存模型\n",
    "    result=val_predict()\n",
    "    print(result)\n",
    "    acc=tacc.get_acc(result)\n",
    "    print(acc)\n",
    "    acc_log.append(acc)\n",
    "    with open('acc.txt','a+') as f:\n",
    "        print(acc_log.index(max(acc_log)),file=f)\n",
    "    \n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
